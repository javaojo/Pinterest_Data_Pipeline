{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d01fa9-e6d9-471f-a67a-8eb3bf116ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bc9021-59d3-4e0e-881a-3a3bd4d7f136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col, when, concat, lit, to_timestamp, regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dee474f-c2d4-4674-9336-11548627eeac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Performing Cleaning on df_pin Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f3dd0d-31b3-4c77-bb1b-42df1e7ede38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def topics_to_dataframe():\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads JSON data from specified topics and returns an array of Spark DataFrames.\n",
    "\n",
    "    The function reads JSON files from different topics (.pin, .geo, .user) located in the mounted S3 bucket.\n",
    "    It disables format checks during the reading of Delta tables to improve performance.\n",
    "\n",
    "    Returns:\n",
    "        List: An array of Spark DataFrames, each containing data from a different topic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable format checks during the reading of Delta tables\n",
    "    spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\", \"false\")\n",
    "    \n",
    "    # Define the list of topics (file extensions)\n",
    "    topics = [\".pin\", \".geo\", \".user\"]\n",
    "    df_array = []\n",
    "\n",
    "    # Iterate through each topic\n",
    "    for file_extension in topics:\n",
    "        # Construct the file location based on the topic\n",
    "        file_location = \"/mnt/databricks-bucket/topics/0ae9e110c9db\" + file_extension + \"/partition=0/*.json\" \n",
    "        # Specify the file type as JSON\n",
    "        file_type = \"json\"\n",
    "        # Request Spark to infer the schema during reading\n",
    "        infer_schema = \"true\"\n",
    "        # Read JSON files from the specified location into a Spark DataFrame\n",
    "        df = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(file_location)\n",
    "        # Append the DataFrame to the df_array\n",
    "        df_array.append(df)\n",
    "\n",
    "    # Return the array of DataFrames for different topics\n",
    "    return df_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f2491d-09c8-4fe5-b03f-0e6474bc04a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An array of stored dataframe topics \n",
    "df_array = topics_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10
     },
     "inputWidgets": {},
     "nuid": "bef58cc3-eba5-4e33-82da-e8f1c555694e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_df_pin(df_pin):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame df_pin by replacing null or irrelevant data with default values.\n",
    "\n",
    "    Parameters:\n",
    "    - df_pin: Input DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary containing default values to replace null or irrelevant data\n",
    "    null_dicts = {\n",
    "        \"description\": \"No description available\",\n",
    "        \"follower_count\": \"User Info Error\",\n",
    "        \"image_src\": \"Image src error.\",\n",
    "        \"poster_name\": \"User Info Error\",\n",
    "        \"tag_list\": \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",\n",
    "        \"title\": \"No Title Data Available\"\n",
    "    }\n",
    "\n",
    "    # Drop any duplicate rows in the dataframe (if needed)\n",
    "    # df_pin = df_pin.dropDuplicates(subset=[\"unique_id\"])\n",
    "\n",
    "    # Replace empty and non-relevant data in each column with specified values\n",
    "    for key, value in null_dicts.items():\n",
    "        if key == \"follower_count\":\n",
    "            # Replace specified value with \"0\" in the \"follower_count\" column\n",
    "            df_pin = df_pin.withColumn(key, regexp_replace(key, value, \"0\"))\n",
    "        elif key == \"description\":\n",
    "            # Replace the description column with \"None\" if it starts with the specified value, otherwise keep the original value\n",
    "            df_pin = df_pin.withColumn(key, when(col(key).startswith(value), \"None\").otherwise(col(key)))\n",
    "        else:\n",
    "            # Replace specified value with \"None\" in other columns\n",
    "            df_pin = df_pin.withColumn(key, regexp_replace(key, value, \"None\"))\n",
    "\n",
    "    # Convert follower_count values with \"k\", \"M\", \"B\" suffixes to numeric values\n",
    "    df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "    df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"0000\"))\n",
    "    df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"B\", \"00000\"))\n",
    "    df_pin = df_pin.withColumn(\"follower_count\", df_pin[\"follower_count\"].cast(\"int\"))\n",
    "\n",
    "    # Remove \"Local save in\" from save_location column\n",
    "    df_pin = df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in\", \"\"))\n",
    "\n",
    "    # Rename the \"index\" column to \"ind\"\n",
    "    df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "    # Define the desired order of columns\n",
    "    column_order_pin = [\n",
    "        \"ind\",\n",
    "        \"unique_id\",\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"follower_count\",\n",
    "        \"poster_name\",\n",
    "        \"tag_list\",\n",
    "        \"is_image_or_video\",\n",
    "        \"image_src\",\n",
    "        \"save_location\",\n",
    "        \"category\"\n",
    "    ]\n",
    "\n",
    "    # Select and reorder the columns in the specified order\n",
    "    df_pin = df_pin.select(column_order_pin)\n",
    "\n",
    "    return df_pin\n",
    "\n",
    "# Clean the dataframe and store the result in df_pin\n",
    "df_pin = clean_df_pin(df_array[0].select(*df_array[0].columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87dde37f-8a6a-4b2c-aa7b-5570035b6ada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Performing Cleaning on df_geo Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52204a64-4439-484f-bfd9-e6d1812f5483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_df_geo(df_geo):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame containing geographical data.\n",
    "\n",
    "    Args:\n",
    "        df_geo (DataFrame): Input DataFrame containing geographical data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned DataFrame with specified columns and transformations.\n",
    "    \"\"\"\n",
    "    # Select relevant columns from the input DataFrame\n",
    "    df_geo = df_array[1].select(*df_array[1].columns)\n",
    "    \n",
    "    # Uncomment the line below if duplicate rows need to be dropped\n",
    "    # df_geo = df_geo.dropDuplicates()\n",
    "\n",
    "    # Create a new column \"coordinates\" by combining \"latitude\" and \"longitude\"\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "\n",
    "    # Drop the original \"latitude\" and \"longitude\" columns\n",
    "    df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "    # Convert the \"timestamp\" column to a timestamp type\n",
    "    df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "    # Define the desired order of columns\n",
    "    column_order_geo = [\n",
    "        \"ind\",\n",
    "        \"country\",\n",
    "        \"coordinates\",\n",
    "        \"timestamp\"\n",
    "    ]\n",
    "    \n",
    "    # Select and reorder the columns in the specified order\n",
    "    df_geo = df_geo.select(column_order_geo)\n",
    "    \n",
    "    return df_geo\n",
    "\n",
    "# Clean the geographical DataFrame and store the result in df_geo\n",
    "df_geo = clean_df_geo(df_array[1].select(*df_array[1].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3129edbe-7e40-4d7a-84e0-3e1346858e48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Performing Cleaning on df_user Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ac7545-d8cb-4f90-a99d-4a29fbf2f579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_df_user(df_user):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame containing user data.\n",
    "\n",
    "    Args:\n",
    "        df_user (DataFrame): Input DataFrame containing user data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned DataFrame with specified columns and transformations.\n",
    "    \"\"\"\n",
    "    # Concatenate \"first_name\" and \"last_name\" columns to create a new \"user_name\" column\n",
    "    df_user = df_user.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "\n",
    "    # Drop the original \"first_name\" and \"last_name\" columns\n",
    "    df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "    # Convert the \"date_joined\" column to a timestamp type\n",
    "    df_user = df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "    # Select the desired columns\n",
    "    df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "\n",
    "    return df_user\n",
    "\n",
    "# Clean the user DataFrame and store the result in df_user\n",
    "df_user = clean_df_user(df_array[2].select(*df_array[2].columns))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clean_Data_Batch",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
